<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <title>Flask STT Demo</title>
  <style>
    body { font-family: sans-serif; padding: 2rem; }
    #btn { padding: 1rem 2rem; font-size: 1.2rem; }
    #transcript { margin-top: 1rem; white-space: pre-wrap; }
  </style>
</head>
<body>
  <h1>Hold to Talk</h1>
  <button id="btn">Giữ nút và nói</button>
  <div id="status">Trạng thái: idle</div>
  <div id="transcript"></div>

  <script>
    let mediaRecorder, audioChunks = [];
    const btn = document.getElementById('btn');
    const status = document.getElementById('status');
    const transcriptDiv = document.getElementById('transcript');

    navigator.mediaDevices.getUserMedia({ audio: true })
      .then(stream => {
        mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
        mediaRecorder.addEventListener('dataavailable', e => audioChunks.push(e.data));
        mediaRecorder.addEventListener('stop', () => {
          const blob = new Blob(audioChunks, { type: 'audio/webm' });
          audioChunks = [];
          status.textContent = 'Đang gửi lên server...';
          blobToWave(blob, 16000).then(wavBlob => {
            fetch('/recognize', {
              method: 'POST',
              headers: { 'Content-Type': 'application/octet-stream' },
              body: wavBlob
            })
            .then(res => res.json())
            .then(data => {
              transcriptDiv.textContent = data.transcript;
              status.textContent = 'Idle';
            })
            .catch(() => status.textContent = 'Lỗi kết nối');
          });
        });
      })
      .catch(() => status.textContent = 'Không có quyền microphone');

    btn.addEventListener('mousedown', () => {
      if (mediaRecorder.state === 'inactive') {
        mediaRecorder.start();
        status.textContent = 'Recording…';
      }
    });
    btn.addEventListener('mouseup', () => {
      if (mediaRecorder.state === 'recording') mediaRecorder.stop();
    });

    async function blobToWave(blob, targetSampleRate) {
      const arrayBuffer = await blob.arrayBuffer();
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const decoded = await audioCtx.decodeAudioData(arrayBuffer);
      const offlineCtx = new (window.OfflineAudioContext || window.webkitOfflineAudioContext)(
        decoded.numberOfChannels,
        decoded.duration * targetSampleRate,
        targetSampleRate
      );
      const source = offlineCtx.createBufferSource();
      source.buffer = decoded;
      source.connect(offlineCtx.destination);
      source.start(0);
      const rendered = await offlineCtx.startRendering();
      return new Blob([audioBufferToWav(rendered)], { type: 'application/octet-stream' });
    }

    function audioBufferToWav(buffer) {
      const numCh = buffer.numberOfChannels;
      const length = buffer.length * numCh * 2 + 44;
      const buf = new ArrayBuffer(length);
      const view = new DataView(buf);
      function writeStr(offset, s) { for (let i=0;i<s.length;i++) view.setUint8(offset+i, s.charCodeAt(i)); }
      writeStr(0, 'RIFF'); view.setUint32(4, 36 + buffer.length * numCh * 2, true);
      writeStr(8, 'WAVE'); writeStr(12, 'fmt '); view.setUint32(16,16,true);
      view.setUint16(20,1,true); view.setUint16(22,numCh,true);
      view.setUint32(24,buffer.sampleRate,true);
      view.setUint32(28,buffer.sampleRate*numCh*2,true);
      view.setUint16(32,numCh*2,true); view.setUint16(34,16,true);
      writeStr(36,'data'); view.setUint32(40,buffer.length*numCh*2,true);
      let offset=44;
      const chanData = [];
      for (let c=0;c<numCh;c++) chanData.push(buffer.getChannelData(c));
      for (let i=0;i<buffer.length;i++){
        for (let c=0;c<numCh;c++){
          let s = Math.max(-1,Math.min(1, chanData[c][i]));
          view.setInt16(offset, s<0 ? s*0x8000 : s*0x7FFF, true);
          offset+=2;
        }
      }
      return view;
    }
  </script>
</body>
</html>
